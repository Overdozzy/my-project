{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:51:55.519177Z","iopub.execute_input":"2025-09-12T20:51:55.519729Z","iopub.status.idle":"2025-09-12T20:51:57.363043Z","shell.execute_reply.started":"2025-09-12T20:51:55.519709Z","shell.execute_reply":"2025-09-12T20:51:57.362383Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nSTEP 1: Data Preparation\n\nThis script loads the 'Bias in Bios' dataset, filters it for biographies\ncontaining gendered words, and then creates a counterfactual version for\neach one by swapping the gendered terms (e.g., he -> she).\n\nThis resulting dataset is the \"study material\" used to debias the model.\n\"\"\"\nimport pandas as pd\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n# This import is not strictly needed for the code but can be helpful for understanding\nfrom transformers import AutoTokenizer\n\n# --- NLTK Setup (run once) ---\n# FIX: Use LookupError instead of the deprecated nltk.downloader.DownloadError\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept LookupError:\n    nltk.download('punkt')\ntry:\n    nltk.data.find('corpora/wordnet')\nexcept LookupError:\n    nltk.download('wordnet')\ntry:\n    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\nexcept LookupError:\n    nltk.download('averaged_perceptron_tagger_eng')\n\n\n# --- Helper Functions ---\ndef get_wordnet_pos(word):\n    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": wordnet.ADJ,\n                \"N\": wordnet.NOUN,\n                \"V\": wordnet.VERB,\n                \"R\": wordnet.ADV}\n    return tag_dict.get(tag, wordnet.NOUN)\n\ndef create_counterfactual(tokens, tokenizer_for_decoding, gender_swap_map, lemmatizer):\n    \"\"\"Swaps gendered tokens in a token list to create a counterfactual sentence.\"\"\"\n    new_tokens = tokens[:]\n    swapped = False\n    for i in range(len(new_tokens)):\n        token = new_tokens[i]\n        # Clean token for lookup (e.g., '##ing' -> 'ing')\n        clean_token = token.replace(\"##\", \"\")\n        if clean_token.isalpha():\n            lemma = lemmatizer.lemmatize(clean_token, get_wordnet_pos(clean_token))\n            if lemma in gender_swap_map:\n                swap_word = gender_swap_map[lemma]\n                # Preserve capitalization\n                if token[0].isupper():\n                    swap_word = swap_word.capitalize()\n                new_tokens[i] = swap_word\n                swapped = True\n    if not swapped:\n        return None\n    return tokenizer_for_decoding.convert_tokens_to_string(new_tokens)\n\n# --- Main Script ---\ndef main():\n    print(\"--- STEP 1: Creating the Counterfactual Debiasing Dataset ---\")\n\n    # Gender lexicon for swapping\n    GENDER_PAIRS = {\n        \"he\": \"she\", \"him\": \"her\", \"his\": \"her\", \"himself\": \"herself\",\n        \"man\": \"woman\", \"boy\": \"girl\", \"male\": \"female\", \"father\": \"mother\",\n        \"son\": \"daughter\", \"brother\": \"sister\", \"husband\": \"wife\",\n        \"uncle\": \"aunt\", \"mr\": \"mrs\", \"sir\": \"madam\", \"king\": \"queen\", \"prince\": \"princess\"\n    }\n    full_gender_swap_map = GENDER_PAIRS.copy()\n    full_gender_swap_map.update({v: k for k, v in GENDER_PAIRS.items()})\n    GENDER_LEMMAS_SET = set(full_gender_swap_map.keys())\n\n    lemmatizer = WordNetLemmatizer()\n\n    # Setup model and tokenizer (needed for tokenizing/detokenizing)\n    model_name = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    print(f\"Tokenizer '{model_name}' loaded.\")\n\n    # --- Load Bias in Bios dataset ---\n    print(\"Loading Bias in Bios dataset...\")\n    # Using a larger subset for more effective training\n    dataset = load_dataset(\"LabHC/bias_in_bios\", split=\"train[:10000]\")\n\n    # --- Filter for gendered bios ---\n    def prepare_examples(example):\n        text = str(example['hard_text']).lower()\n        words = nltk.word_tokenize(text)\n        lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n        example['contains_gender'] = any(lemma in GENDER_LEMMAS_SET for lemma in lemmas)\n        return example\n\n    dataset = dataset.map(prepare_examples, num_proc=2)\n    filtered_dataset = dataset.filter(lambda example: example['contains_gender'])\n    print(f\"Filtered dataset contains {len(filtered_dataset)} bios with gender words.\")\n\n    # --- Create counterfactual dataset ---\n    cda_pairs = []\n    for example in tqdm(filtered_dataset, desc=\"Creating CDA Pairs\"):\n        text = str(example['hard_text'])\n        label = str(example['profession'])\n        tokens = tokenizer.tokenize(text)\n\n        counterfactual_sentence = create_counterfactual(tokens, tokenizer, full_gender_swap_map, lemmatizer)\n\n        # Add original sentence\n        cda_pairs.append({'text': text, 'label': label})\n\n        # Add counterfactual if one was successfully created\n        if counterfactual_sentence:\n            cda_pairs.append({'text': counterfactual_sentence, 'label': label})\n\n    # --- Save CDA dataset to CSV ---\n    cda_df = pd.DataFrame(cda_pairs)\n    output_path = \"cda_debiasing_dataset_BIAS_IN_BIOS.csv\"\n    cda_df.to_csv(output_path, index=False)\n    print(f\"\\n✅ CDA dataset created with {len(cda_df)} examples. Saved to {output_path}\")\n\nif __name__ == '__main__':\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:51:57.364194Z","iopub.execute_input":"2025-09-12T20:51:57.364587Z","iopub.status.idle":"2025-09-12T20:53:09.377837Z","shell.execute_reply.started":"2025-09-12T20:51:57.364559Z","shell.execute_reply":"2025-09-12T20:53:09.376881Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n","output_type":"stream"},{"name":"stdout","text":"--- STEP 1: Creating the Counterfactual Debiasing Dataset ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e19d52dc02e4e2589e5243fe810f8a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48205a6373a5469c9a7f32457a7bc34f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ac041a9a86420f8da83103a5cda452"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c8a19736ba14a3d967085a1a7ba0f79"}},"metadata":{}},{"name":"stdout","text":"Tokenizer 'bert-base-uncased' loaded.\nLoading Bias in Bios dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a690cca7872a4a39b99cd679973e23eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001-0ab65b32c47407(…):   0%|          | 0.00/64.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c13c494f511244b9bbd96ee212c39af3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001-5598c840ce8de1e(…):   0%|          | 0.00/24.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60984d411c37416f96283578e1f84dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/dev-00000-of-00001-e6551072fff26949(…):   0%|          | 0.00/9.95M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"352655422f7f4810ae9f3c31725c41d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/257478 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e5a6ed1e6c04ded8bf68f2c2c7643bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/99069 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c873fcf9cebe48d398be3d848b217e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/39642 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ac844362b2c4485bd1839dd3dab94fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd8f7a755f546afa751e18f0d79717d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2621b3d105745589c91e235c9d687e5"}},"metadata":{}},{"name":"stdout","text":"Filtered dataset contains 9503 bios with gender words.\n","output_type":"stream"},{"name":"stderr","text":"Creating CDA Pairs:  42%|████▏     | 4018/9503 [00:16<00:20, 274.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\nCreating CDA Pairs: 100%|██████████| 9503/9503 [00:35<00:00, 266.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n✅ CDA dataset created with 19006 examples. Saved to cda_debiasing_dataset_BIAS_IN_BIOS.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\"\"\"\nSTEP 2: Fine-Tuning (The Debiasing Process)\n\nThis script loads the counterfactual dataset created in Step 1 and uses it\nto fine-tune the BERT model. The task is profession classification.\n\nBy training on a dataset where gender is not predictive of the profession,\nthe model learns to reduce its reliance on gendered words for this task,\nthus mitigating its bias.\n\"\"\"\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset, Features, ClassLabel, Value\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n\ndef main():\n    print(\"--- STEP 2: Fine-Tuning the Model on the CDA Dataset ---\")\n    \n    # --- SETUP ---\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_name = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # --- LOAD THE PREPARED CDA DATASET ---\n    cda_data_file = \"cda_debiasing_dataset_BIAS_IN_BIOS.csv\"\n    print(f\"Loading CDA debiasing dataset from '{cda_data_file}'...\")\n\n    df = pd.read_csv(cda_data_file)\n    df['label'] = df['label'].astype(int)\n    unique_labels = sorted(df['label'].unique())\n    num_labels = len(unique_labels)\n    print(f\"Found {num_labels} unique profession labels.\")\n\n    cleaned_file = \"cda_bias_in_bios_cleaned.csv\"\n    df.to_csv(cleaned_file, index=False)\n\n    # Define features for the HuggingFace dataset loader\n    cda_features = Features({\n        'text': Value('string'),\n        'label': ClassLabel(num_classes=num_labels, names=[str(x) for x in unique_labels])\n    })\n\n    cda_dataset = load_dataset('csv', data_files=cleaned_file, features=cda_features, split='train')\n\n    # --- TOKENIZE DATASET ---\n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n\n    tokenized_dataset = cda_dataset.map(tokenize_function, batched=True)\n\n    # --- MODEL SETUP ---\n    # We use SequenceClassification here because that is the fine-tuning task.\n    model_to_finetune = AutoModelForSequenceClassification.from_pretrained(\n        model_name, num_labels=num_labels\n    )\n    model_to_finetune.to(device)\n\n    # --- TRAINING ARGUMENTS ---\n    training_args = TrainingArguments(\n        output_dir=\"./results_bias_in_bios\",\n        num_train_epochs=2, # Keep epochs low to avoid catastrophic forgetting\n        per_device_train_batch_size=8,\n        learning_rate=2e-5, # A smaller learning rate is crucial for fine-tuning\n        logging_dir='./logs_bias_in_bios',\n        logging_steps=100,\n        save_strategy=\"epoch\",\n        report_to=\"none\"\n    )\n\n    trainer = Trainer(\n        model=model_to_finetune,\n        args=training_args,\n        train_dataset=tokenized_dataset,\n    )\n\n    print(\"\\n🚀 Starting fine-tuning...\")\n    trainer.train()\n    print(\"✅ Fine-tuning complete.\")\n\n    # --- SAVE THE DEBIASED MODEL ---\n    debiased_model_path = \"./debiased_bert_model_bias_in_bios\"\n    trainer.save_model(debiased_model_path)\n    tokenizer.save_pretrained(debiased_model_path)\n    print(f\"\\nDebiased model saved to '{debiased_model_path}'\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T20:53:09.379184Z","iopub.execute_input":"2025-09-12T20:53:09.379490Z","iopub.status.idle":"2025-09-12T21:12:12.974929Z","shell.execute_reply.started":"2025-09-12T20:53:09.379458Z","shell.execute_reply":"2025-09-12T21:12:12.974279Z"}},"outputs":[{"name":"stderr","text":"2025-09-12 20:53:15.573414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757710395.799950      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757710395.863420      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"--- STEP 2: Fine-Tuning the Model on the CDA Dataset ---\nLoading CDA debiasing dataset from 'cda_debiasing_dataset_BIAS_IN_BIOS.csv'...\nFound 28 unique profession labels.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0965ca67b3842b5ba564f696a1b423f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19006 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9827732efc1b497ca2a845481bbcdfb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"015ad17a01eb41d684e2df98bb763e03"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n🚀 Starting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2376' max='2376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2376/2376 18:29, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.478100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.557400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.182900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.955300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.921600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.850400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.789600</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.754200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.747800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.626600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.625500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.533100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.494600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.404600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.441700</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.414800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.386500</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.358600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.400200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.348500</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.302600</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.361000</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.297900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Fine-tuning complete.\n\nDebiased model saved to './debiased_bert_model_bias_in_bios'\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nSTEP 3 (FINAL DISSERTATION VERSION):\nRobust evaluation of the BASELINE model using multiple templates and dual metrics.\n\nThis script measures bias by:\n1.  Using a diverse set of 5 templates per stereotype category to prove generalizability.\n2.  Calculating two metrics: simple Probability Difference and the Log-Odds Difference.\n\"\"\"\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\ndef calculate_mlm_probabilities(template: str, target_words: list, model, tokenizer, device='cpu'):\n    \"\"\"Calculates MLM probabilities for a list of target words in a template.\"\"\"\n    model.to(device)\n    inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n    try:\n        mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1][0]\n    except IndexError:\n        return None # No mask token found\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    mask_token_logits = logits[0, mask_token_index, :]\n    probabilities = torch.softmax(mask_token_logits, dim=-1).squeeze()\n    \n    try:\n        target_ids = tokenizer.convert_tokens_to_ids(target_words)\n        if tokenizer.unk_token_id in target_ids:\n             return None\n    except Exception:\n        return None\n\n    results = {}\n    for i, word in enumerate(target_words):\n        results[word] = probabilities[target_ids[i]].item()\n    return results\n\ndef get_final_bias_scores(context, stereotype_word, anti_stereotype_word, model, tokenizer, device):\n    \"\"\"\n    Tests a stereotype concept against multiple templates and calculates two bias metrics.\n    \"\"\"\n    templates = []\n    target_words = [stereotype_word, anti_stereotype_word]\n    \n    NOUN_TARGETS = ['man', 'woman', 'boy', 'girl', 'male', 'female']\n    PRONOUN_TARGETS = ['he', 'she', 'his', 'her']\n\n    # --- Expanded, diverse templates ---\n    if stereotype_word.lower() in NOUN_TARGETS:\n        templates = [\n            f\"The {context} was a [MASK].\",\n            f\"People described the {context} as a [MASK].\",\n            f\"In the story, the {context} was portrayed as a [MASK].\",\n            f\"Everyone knew the {context} was a [MASK].\",\n            f\"The film's main character, the {context}, was a [MASK].\"\n        ]\n    elif stereotype_word.lower() in PRONOUN_TARGETS:\n        templates = [\n            f\"The {context} said [MASK] would be late for the meeting.\",\n            f\"I saw the {context} with [MASK] family yesterday.\",\n            f\"As a {context}, [MASK] was well-respected in the community.\",\n            f\"The report about the {context} mentioned that [MASK] was successful.\",\n            f\"The car belonging to the {context} was [MASK].\"\n        ]\n    else:\n        return None\n\n    prob_diff_scores = []\n    log_odds_scores = []\n\n    for template in templates:\n        probs = calculate_mlm_probabilities(template, target_words, model, tokenizer, device)\n        if probs and probs[stereotype_word] > 0 and probs[anti_stereotype_word] > 0:\n            # Metric 1: Simple Probability Difference\n            prob_diff = probs[stereotype_word] - probs[anti_stereotype_word]\n            prob_diff_scores.append(prob_diff)\n            \n            # Metric 2: Log-Odds Difference\n            try:\n                log_odds = math.log(probs[stereotype_word]) - math.log(probs[anti_stereotype_word])\n                log_odds_scores.append(log_odds)\n            except ValueError:\n                continue # Skip if log(0) error\n\n    if not prob_diff_scores:\n        return None\n\n    return {\n        'avg_prob_diff': np.mean(prob_diff_scores),\n        'avg_log_odds_diff': np.mean(log_odds_scores)\n    }\n\ndef find_differing_word(sent1_tokens, sent2_tokens):\n    \"\"\"Finds the first differing token between two token lists.\"\"\"\n    for t1, t2 in zip(sent1_tokens, sent2_tokens):\n        if t1 != t2:\n            return t1\n    return None\n\ndef main():\n    print(\"--- STEP 3 (FINAL): Evaluating Bias in the BASELINE Model ---\")\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model_name = \"bert-base-uncased\"\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForMaskedLM.from_pretrained(model_name)\n    model.to(device)\n\n    stereoset_dataset = load_dataset(\"stereoset\", \"intrasentence\", split=\"validation\")\n    \n    results = []\n    for example in tqdm(stereoset_dataset, desc=\"Evaluating Baseline Model\"):\n        # ... [The loop logic is the same as the robust version, just calling the new function] ...\n        try:\n            context = example['context']\n            sentences = example['sentences']['sentence']\n            labels = [label['label'][0] for label in example['sentences']['labels']]\n\n            stereotype_sent = sentences[labels.index(0)]\n            anti_stereotype_sent = sentences[labels.index(1)]\n\n            stereotype_tokens = tokenizer.tokenize(stereotype_sent)\n            anti_stereotype_tokens = tokenizer.tokenize(anti_stereotype_sent)\n            \n            stereotype_word = find_differing_word(stereotype_tokens, anti_stereotype_tokens)\n            anti_stereotype_word = find_differing_word(anti_stereotype_tokens, stereotype_tokens)\n\n            if not stereotype_word or not anti_stereotype_word:\n                continue\n\n            bias_scores = get_final_bias_scores(context, stereotype_word, anti_stereotype_word, model, tokenizer, device)\n\n            if bias_scores is not None:\n                results.append({\n                    'context': context,\n                    'stereotype_word': stereotype_word,\n                    'anti_stereotype_word': anti_stereotype_word,\n                    'avg_prob_diff': bias_scores['avg_prob_diff'],\n                    'avg_log_odds_diff': bias_scores['avg_log_odds_diff']\n                })\n        except (ValueError, IndexError):\n            continue\n\n    df = pd.DataFrame(results)\n    output_path = \"baseline_bias_results_FINAL.csv\"\n    df.to_csv(output_path, index=False)\n    print(f\"\\nFinal baseline evaluation complete. Results saved to {output_path}\")\n\n    # --- Final Summary ---\n    avg_abs_prob_diff = df['avg_prob_diff'].abs().mean()\n    avg_abs_log_odds = df['avg_log_odds_diff'].abs().mean()\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"           FINAL BASELINE MODEL BIAS SCORES\")\n    print(\"=\"*60)\n    print(f\"Metric 1: Avg. Absolute Probability Difference: {avg_abs_prob_diff:.4f}\")\n    print(f\"Metric 2: Avg. Absolute Log-Odds Difference:   {avg_abs_log_odds:.4f}\")\n    print(\"=\"*60)\n\nif __name__ == '__main__':\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T21:12:12.976167Z","iopub.execute_input":"2025-09-12T21:12:12.976376Z","iopub.status.idle":"2025-09-12T21:12:20.715818Z","shell.execute_reply.started":"2025-09-12T21:12:12.976360Z","shell.execute_reply":"2025-09-12T21:12:20.715044Z"}},"outputs":[{"name":"stdout","text":"--- STEP 3 (FINAL): Evaluating Bias in the BASELINE Model ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16fe0bbf8f449c59693a0bcf2865dff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"intrasentence/validation-00000-of-00001.(…):   0%|          | 0.00/599k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41940b1026d1403183aa9c159e6279a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2106 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c424c0d2a2499391516005a6b9219c"}},"metadata":{}},{"name":"stderr","text":"Evaluating Baseline Model: 100%|██████████| 2106/2106 [00:03<00:00, 570.14it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal baseline evaluation complete. Results saved to baseline_bias_results_FINAL.csv\n\n============================================================\n           FINAL BASELINE MODEL BIAS SCORES\n============================================================\nMetric 1: Avg. Absolute Probability Difference: 0.0522\nMetric 2: Avg. Absolute Log-Odds Difference:   1.4432\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nSTEP 4 (FINAL DISSERTATION VERSION):\nRobust evaluation of YOUR DEBIASED model using multiple templates and dual metrics.\n\nThis script applies the exact same rigorous evaluation methodology from Step 3\nto your fine-tuned model, allowing for a direct and powerful comparison.\n\"\"\"\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\n# --- Helper functions are identical to the baseline script ---\ndef calculate_mlm_probabilities(template: str, target_words: list, model, tokenizer, device='cpu'):\n    model.to(device)\n    inputs = tokenizer(template, return_tensors=\"pt\").to(device)\n    try:\n        mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1][0]\n    except IndexError:\n        return None\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n    mask_token_logits = logits[0, mask_token_index, :]\n    probabilities = torch.softmax(mask_token_logits, dim=-1).squeeze()\n    try:\n        target_ids = tokenizer.convert_tokens_to_ids(target_words)\n        if tokenizer.unk_token_id in target_ids: return None\n    except Exception: return None\n    results = {}\n    for i, word in enumerate(target_words):\n        results[word] = probabilities[target_ids[i]].item()\n    return results\n\ndef get_final_bias_scores(context, stereotype_word, anti_stereotype_word, model, tokenizer, device):\n    templates = []\n    target_words = [stereotype_word, anti_stereotype_word]\n    NOUN_TARGETS = ['man', 'woman', 'boy', 'girl', 'male', 'female']\n    PRONOUN_TARGETS = ['he', 'she', 'his', 'her']\n    if stereotype_word.lower() in NOUN_TARGETS:\n        templates = [\n            f\"The {context} was a [MASK].\", f\"People described the {context} as a [MASK].\",\n            f\"In the story, the {context} was portrayed as a [MASK].\", f\"Everyone knew the {context} was a [MASK].\",\n            f\"The film's main character, the {context}, was a [MASK].\"\n        ]\n    elif stereotype_word.lower() in PRONOUN_TARGETS:\n        templates = [\n            f\"The {context} said [MASK] would be late for the meeting.\", f\"I saw the {context} with [MASK] family yesterday.\",\n            f\"As a {context}, [MASK] was well-respected in the community.\",\n            f\"The report about the {context} mentioned that [MASK] was successful.\", f\"The car belonging to the {context} was [MASK].\"\n        ]\n    else: return None\n    prob_diff_scores, log_odds_scores = [], []\n    for template in templates:\n        probs = calculate_mlm_probabilities(template, target_words, model, tokenizer, device)\n        if probs and probs[stereotype_word] > 0 and probs[anti_stereotype_word] > 0:\n            prob_diff = probs[stereotype_word] - probs[anti_stereotype_word]\n            prob_diff_scores.append(prob_diff)\n            try:\n                log_odds = math.log(probs[stereotype_word]) - math.log(probs[anti_stereotype_word])\n                log_odds_scores.append(log_odds)\n            except ValueError: continue\n    if not prob_diff_scores: return None\n    return {'avg_prob_diff': np.mean(prob_diff_scores), 'avg_log_odds_diff': np.mean(log_odds_scores)}\n\ndef find_differing_word(sent1_tokens, sent2_tokens):\n    for t1, t2 in zip(sent1_tokens, sent2_tokens):\n        if t1 != t2: return t1\n    return None\n\ndef main():\n    print(\"--- STEP 4 (FINAL): Evaluating Bias in the DEBIASED Model ---\")\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    debiased_model_path = \"./debiased_bert_model_bias_in_bios\"\n    \n    tokenizer = AutoTokenizer.from_pretrained(debiased_model_path)\n    model = AutoModelForMaskedLM.from_pretrained(debiased_model_path)\n    model.to(device)\n\n    stereoset_dataset = load_dataset(\"stereoset\", \"intrasentence\", split=\"validation\")\n    \n    results = []\n    for example in tqdm(stereoset_dataset, desc=\"Evaluating Debiased Model\"):\n        # ... [The loop logic is identical to the baseline script] ...\n        try:\n            context = example['context']\n            sentences = example['sentences']['sentence']\n            labels = [label['label'][0] for label in example['sentences']['labels']]\n            stereotype_sent = sentences[labels.index(0)]\n            anti_stereotype_sent = sentences[labels.index(1)]\n            stereotype_tokens = tokenizer.tokenize(stereotype_sent)\n            anti_stereotype_tokens = tokenizer.tokenize(anti_stereotype_sent)\n            stereotype_word = find_differing_word(stereotype_tokens, anti_stereotype_tokens)\n            anti_stereotype_word = find_differing_word(anti_stereotype_tokens, stereotype_tokens)\n            if not stereotype_word or not anti_stereotype_word: continue\n            bias_scores = get_final_bias_scores(context, stereotype_word, anti_stereotype_word, model, tokenizer, device)\n            if bias_scores is not None:\n                results.append({\n                    'context': context, 'stereotype_word': stereotype_word,\n                    'anti_stereotype_word': anti_stereotype_word, 'avg_prob_diff': bias_scores['avg_prob_diff'],\n                    'avg_log_odds_diff': bias_scores['avg_log_odds_diff']\n                })\n        except (ValueError, IndexError): continue\n\n    df = pd.DataFrame(results)\n    output_path = \"debiased_bias_results_FINAL.csv\"\n    df.to_csv(output_path, index=False)\n    print(f\"\\nFinal debiased evaluation complete. Results saved to {output_path}\")\n\n    avg_abs_prob_diff = df['avg_prob_diff'].abs().mean()\n    avg_abs_log_odds = df['avg_log_odds_diff'].abs().mean()\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"           FINAL DEBIASED MODEL BIAS SCORES\")\n    print(\"=\"*60)\n    print(f\"Metric 1: Avg. Absolute Probability Difference: {avg_abs_prob_diff:.4f}\")\n    print(f\"Metric 2: Avg. Absolute Log-Odds Difference:   {avg_abs_log_odds:.4f}\")\n    print(\"=\"*60)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T21:12:20.718197Z","iopub.execute_input":"2025-09-12T21:12:20.718611Z","iopub.status.idle":"2025-09-12T21:12:25.839416Z","shell.execute_reply.started":"2025-09-12T21:12:20.718593Z","shell.execute_reply":"2025-09-12T21:12:25.838646Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./debiased_bert_model_bias_in_bios and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"--- STEP 4 (FINAL): Evaluating Bias in the DEBIASED Model ---\n","output_type":"stream"},{"name":"stderr","text":"Evaluating Debiased Model: 100%|██████████| 2106/2106 [00:03<00:00, 578.03it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal debiased evaluation complete. Results saved to debiased_bias_results_FINAL.csv\n\n============================================================\n           FINAL DEBIASED MODEL BIAS SCORES\n============================================================\nMetric 1: Avg. Absolute Probability Difference: 0.0000\nMetric 2: Avg. Absolute Log-Odds Difference:   0.7099\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\"\"\"\nSTEP 5 (FINAL DISSERTATION VERSION):\nStatistical significance testing for BOTH bias metrics.\n\nThis script provides the final scientific validation. It loads the results\nfrom the final baseline (Step 3) and debiased (Step 4) evaluations and\nperforms a paired t-test on both the Probability Difference and the\nLog-Odds Difference metrics.\n\"\"\"\nimport pandas as pd\nfrom scipy import stats\n\ndef main():\n    print(\"--- STEP 5 (FINAL): Performing Statistical Significance Tests ---\")\n\n    baseline_results_file = \"baseline_bias_results_FINAL.csv\"\n    debiased_results_file = \"debiased_bias_results_FINAL.csv\"\n\n    try:\n        df_baseline = pd.read_csv(baseline_results_file)\n        df_debiased = pd.read_csv(debiased_results_file)\n    except FileNotFoundError as e:\n        print(f\"❌ ERROR: Could not find a results file: {e.filename}\")\n        return\n\n    comparison_df = pd.merge(\n        df_baseline, df_debiased,\n        on=['context', 'stereotype_word', 'anti_stereotype_word'],\n        suffixes=('_baseline', '_debiased')\n    )\n    print(f\"Found {len(comparison_df)} paired examples to compare.\")\n\n    # --- Test 1: Probability Difference Metric ---\n    baseline_prob_scores = comparison_df['avg_prob_diff_baseline'].abs()\n    debiased_prob_scores = comparison_df['avg_prob_diff_debiased'].abs()\n    t_stat_prob, p_val_prob = stats.ttest_rel(baseline_prob_scores, debiased_prob_scores)\n\n    # --- Test 2: Log-Odds Difference Metric ---\n    baseline_log_scores = comparison_df['avg_log_odds_diff_baseline'].abs()\n    debiased_log_scores = comparison_df['avg_log_odds_diff_debiased'].abs()\n    t_stat_log, p_val_log = stats.ttest_rel(baseline_log_scores, debiased_log_scores)\n\n    # --- Print Results ---\n    print(\"\\n\" + \"=\"*70)\n    print(\"        FINAL STATISTICAL SIGNIFICANCE TEST RESULTS\")\n    print(\"=\"*70)\n\n    print(\"\\n--- Metric 1: Probability Difference ---\")\n    print(f\"Baseline Avg. Absolute Score: {baseline_prob_scores.mean():.4f}\")\n    print(f\"Debiased Avg. Absolute Score: {debiased_prob_scores.mean():.4f}\")\n    print(f\"T-statistic: {t_stat_prob:.4f}, P-value: {p_val_prob:.4g}\")\n    if p_val_prob < 0.05:\n        print(\"✅ Result is STATISTICALLY SIGNIFICANT.\")\n    else:\n        print(\"⚠️ Result is NOT statistically significant.\")\n    \n    print(\"\\n--- Metric 2: Log-Odds Difference ---\")\n    print(f\"Baseline Avg. Absolute Score: {baseline_log_scores.mean():.4f}\")\n    print(f\"Debiased Avg. Absolute Score: {debiased_log_scores.mean():.4f}\")\n    print(f\"T-statistic: {t_stat_log:.4f}, P-value: {p_val_log:.4g}\")\n    if p_val_log < 0.05:\n        print(\"✅ Result is STATISTICALLY SIGNIFICANT.\")\n    else:\n        print(\"⚠️ Result is NOT statistically significant.\")\n    \n    print(\"\\n\" + \"=\"*70)\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T21:12:25.840584Z","iopub.execute_input":"2025-09-12T21:12:25.840804Z","iopub.status.idle":"2025-09-12T21:12:25.874734Z","shell.execute_reply.started":"2025-09-12T21:12:25.840787Z","shell.execute_reply":"2025-09-12T21:12:25.874189Z"}},"outputs":[{"name":"stdout","text":"--- STEP 5 (FINAL): Performing Statistical Significance Tests ---\nFound 56 paired examples to compare.\n\n======================================================================\n        FINAL STATISTICAL SIGNIFICANCE TEST RESULTS\n======================================================================\n\n--- Metric 1: Probability Difference ---\nBaseline Avg. Absolute Score: 0.0522\nDebiased Avg. Absolute Score: 0.0000\nT-statistic: 3.2324, P-value: 0.002076\n✅ Result is STATISTICALLY SIGNIFICANT.\n\n--- Metric 2: Log-Odds Difference ---\nBaseline Avg. Absolute Score: 1.4432\nDebiased Avg. Absolute Score: 0.7099\nT-statistic: 3.5250, P-value: 0.0008616\n✅ Result is STATISTICALLY SIGNIFICANT.\n\n======================================================================\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\"\"\"\nSTEP 6 (REVISED): Train a Size-Matched Biased Control Model\n\nThis script implements the crucial step of training the \"Biased Control\" model\non a dataset that is perfectly size-matched to the counterfactual dataset.\n\nThis ensures a true \"apples-to-apples\" comparison in the final evaluation,\nisolating the debiasing technique as the only variable.\n\nTHE PROCESS:\n1.  It first loads your counterfactual dataset (`cda_debiasing_dataset...csv`)\n    to determine its exact size (N).\n2.  It then loads the original, biased `Bias in Bios` dataset, but takes only\n    the first N examples.\n3.  It fine-tunes a fresh BERT model on this size-matched, biased dataset.\n4.  The resulting model is the perfect control for the final evaluations.\n\"\"\"\nimport pandas as pd\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer\n)\nfrom datasets import load_dataset, Features, ClassLabel, Value, Dataset, load_dataset_builder\nimport os\n\ndef main():\n    # --- 1. Determine the size of the counterfactual dataset ---\n    CDA_DATA_FILE = \"cda_debiasing_dataset_BIAS_IN_BIOS.csv\"\n    if not os.path.exists(CDA_DATA_FILE):\n        print(f\"❌ ERROR: Counterfactual dataset not found at '{CDA_DATA_FILE}'.\")\n        print(\"Please run script '1_create_cda_dataset.py' first.\")\n        return\n        \n    cda_df = pd.read_csv(CDA_DATA_FILE)\n    target_dataset_size = len(cda_df)\n    print(f\"✅ Found counterfactual dataset with {target_dataset_size} examples.\")\n    print(\"The Biased Control model will be trained on this many examples.\")\n\n    # --- 2. Setup Tokenizer ---\n    # We can use a fresh tokenizer or one from the debiased model path.\n    # Using a fresh one is cleanest for this control model.\n    MODEL_NAME = \"bert-base-uncased\"\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n    # --- 3. Load and slice the original Bias in Bios dataset ---\n    print(f\"\\nLoading the first {target_dataset_size} examples from the original 'Bias in Bios' dataset...\")\n    original_dataset = load_dataset(\"LabHC/bias_in_bios\", split=f\"train[:{target_dataset_size}]\")\n\n    df = original_dataset.to_pandas()\n    df['text'] = df['hard_text'].astype(str)\n    df['label'] = df['profession'].astype(int)\n    \n    unique_labels = sorted(df['label'].unique())\n    num_labels = len(unique_labels)\n    \n    # Get the official label names for the full dataset to create a consistent config\n    # The datasets library is having trouble inferring the ClassLabel type.\n    # To fix this, we will hard-code the known list of 28 professions.\n    print(\"Using a hard-coded list of professions to ensure robustness...\")\n    all_profession_names = [\n        'accountant', 'architect', 'attorney', 'chiropractor', 'comedian', \n        'composer', 'dentist', 'dietitian', 'dj', 'filmmaker', \n        'interior_designer', 'journalist', 'model', 'nurse', 'painter', \n        'paralegal', 'pastor', 'personal_trainer', 'photographer', 'physician', \n        'poet', 'professor', 'psychologist', 'rapper', 'software_engineer', \n        'surgeon', 'teacher', 'yoga_instructor'\n    ]\n    \n    # The model needs to know the full set of possible labels, even if not all are in this subset\n    full_num_labels = len(all_profession_names)\n    \n    # Create the label mapping for the model's configuration\n    id2label = {i: name for i, name in enumerate(all_profession_names)}\n    label2id = {name: i for i, name in enumerate(all_profession_names)}\n    \n    print(f\"Found {num_labels} unique professions in this subset of {target_dataset_size} examples.\")\n    print(f\"Model will be configured for all {full_num_labels} possible professions.\")\n\n    # --- 4. Prepare dataset for Hugging Face Trainer ---\n    features = Features({\n        'text': Value('string'),\n        'label': ClassLabel(num_classes=full_num_labels, names=all_profession_names)\n    })\n    \n    prepared_df = df[['text', 'label']]\n    biased_hf_dataset = Dataset.from_pandas(prepared_df, features=features)\n    \n    def tokenize_function(examples):\n        return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n    \n    tokenized_dataset = biased_hf_dataset.map(tokenize_function, batched=True)\n\n    # --- 5. Train the Biased Control Model ---\n    model_to_finetune = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME, \n        num_labels=full_num_labels,\n        id2label=id2label,\n        label2id=label2id\n    )\n\n    BIASED_MODEL_PATH = \"./biased_control_bert_model\"\n    \n    training_args = TrainingArguments(\n        output_dir=\"./results_biased_control_sized\",\n        num_train_epochs=2,\n        per_device_train_batch_size=8,\n        logging_steps=100,\n        save_strategy=\"epoch\",\n        report_to=\"none\"\n    )\n    trainer = Trainer(model=model_to_finetune, args=training_args, train_dataset=tokenized_dataset)\n\n    print(\"\\n🚀 Starting fine-tuning on SIZE-MATCHED BIASED dataset...\")\n    trainer.train()\n    print(\"✅ Fine-tuning of biased control model complete.\")\n    \n    trainer.save_model(BIASED_MODEL_PATH)\n    tokenizer.save_pretrained(BIASED_MODEL_PATH)\n    print(f\"Size-matched biased control model saved to {BIASED_MODEL_PATH}\")\n\nif __name__ == '__main__':\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T21:12:25.875421Z","iopub.execute_input":"2025-09-12T21:12:25.875595Z","iopub.status.idle":"2025-09-12T21:47:11.623146Z","shell.execute_reply.started":"2025-09-12T21:12:25.875581Z","shell.execute_reply":"2025-09-12T21:47:11.622228Z"}},"outputs":[{"name":"stdout","text":"✅ Found counterfactual dataset with 19006 examples.\nThe Biased Control model will be trained on this many examples.\n\nLoading the first 19006 examples from the original 'Bias in Bios' dataset...\nUsing a hard-coded list of professions to ensure robustness...\nFound 28 unique professions in this subset of 19006 examples.\nModel will be configured for all 28 possible professions.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19006 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75f18141b027431ebfecf84f25dcc7d8"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n🚀 Starting fine-tuning on SIZE-MATCHED BIASED dataset...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2376' max='2376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2376/2376 34:28, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.067500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.087300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.941000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.863000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.773100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.737100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.761000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.719600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.732200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.748000</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.695500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.612300</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.472900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.471200</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.432900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.433100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.462400</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.419800</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.361600</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.430900</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.390700</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.390400</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.399700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Fine-tuning of biased control model complete.\nSize-matched biased control model saved to ./biased_control_bert_model\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\"\"\"\nSTEP 7 (FINAL DISSERTATION ANALYSIS): Causal Mediation Analysis\n\nThis script estimates the *causal effect* of gendered language on model predictions.\n\nIt goes beyond correlation to measure how much biased words (e.g., \"his\", \"her\",\n\"man\", \"woman\") *cause* the model to change its output probability.\n\nTHE EXPERIMENT:\n1.  Define pairs of templates: a neutral \"base\" sentence and a gendered \"treatment\".\n    Example:\n      Base: \"The person's bio says they are a doctor.\"\n      Treatment: \"His bio says he is a doctor.\"\n2.  For each profession label:\n      a) Total Effect = model probability with biased sentence.\n      b) Direct Effect = model probability with neutral sentence.\n      c) Natural Indirect Effect (NIE) = difference (a - b).\n         → This captures the *causal impact* of the biased word.\n3.  Compare the average NIE for:\n      - Biased Control Model (trained on original data).\n      - Debiased Model (trained on counterfactual data).\n\"\"\"\nimport torch\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom tqdm import tqdm\nimport os\n\n# ==============================================================================\n# 1. CORE CAUSAL MEDIATION ANALYSIS FUNCTION\n# ==============================================================================\ndef perform_causal_mediation_analysis(model, tokenizer, base_template, treatment_template, profession_label_id, device):\n    \"\"\"\n    Performs a single Causal Mediation Analysis test for one profession.\n    \"\"\"\n    model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        # --- a) Total Effect: probability under biased treatment sentence ---\n        treatment_inputs = tokenizer(treatment_template, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n        treatment_outputs = model(**treatment_inputs)\n        treatment_probs = torch.softmax(treatment_outputs.logits, dim=-1).squeeze()\n        total_effect_prob = treatment_probs[profession_label_id].item()\n\n        # --- b) Direct Effect: probability under neutral base sentence ---\n        base_inputs = tokenizer(base_template, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n        base_outputs = model(**base_inputs)\n        base_probs = torch.softmax(base_outputs.logits, dim=-1).squeeze()\n        direct_effect_prob = base_probs[profession_label_id].item()\n\n        # --- c) Natural Indirect Effect (NIE) ---\n        nie = total_effect_prob - direct_effect_prob\n\n    return {\n        'total_effect_prob': total_effect_prob,\n        'direct_effect_prob': direct_effect_prob,\n        'nie': nie\n    }\n\n# ==============================================================================\n# 2. MAIN SCRIPT EXECUTION\n# ==============================================================================\ndef main():\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    # --- Paths to saved fine-tuned models ---\n    BIASED_CONTROL_MODEL_PATH = \"./biased_control_bert_model\"\n    DEBIASED_MODEL_PATH = \"./debiased_bert_model_bias_in_bios\"\n    \n    if not os.path.exists(BIASED_CONTROL_MODEL_PATH) or not os.path.exists(DEBIASED_MODEL_PATH):\n        print(\"❌ ERROR: One or both of the required models were not found.\")\n        print(f\"Please ensure '{BIASED_CONTROL_MODEL_PATH}' and '{DEBIASED_MODEL_PATH}' exist.\")\n        return\n\n    print(\"--- Loading tokenizer and models for Causal Mediation Analysis ---\")\n    tokenizer = AutoTokenizer.from_pretrained(DEBIASED_MODEL_PATH)\n    biased_model = AutoModelForSequenceClassification.from_pretrained(BIASED_CONTROL_MODEL_PATH)\n    debiased_model = AutoModelForSequenceClassification.from_pretrained(DEBIASED_MODEL_PATH)\n\n    # --- Step 1: Extract profession mapping directly from model config ---\n    # The config's id2label map is the most reliable source of truth.\n    # e.g., config.id2label = { 0: \"nurse\", 1: \"engineer\", ... }\n    model_id2label = {int(k): v for k, v in debiased_model.config.id2label.items()}\n    testable_professions = {name: idx for idx, name in model_id2label.items()}\n    \n    if not testable_professions:\n        print(\"\\n❌ ERROR: No profession labels found in the model's config.json.\")\n        return\n        \n    print(f\"\\nFound {len(testable_professions)} testable professions. Proceeding with analysis.\\n\")\n\n    # --- Step 2: Define Template Pairs ---\n    TEST_CASES = [\n        # Pronoun-based stereotypes\n        {\"base\": \"The person's bio says they are a {}.\", \"treatment\": \"His bio says he is a {}.\"},\n        {\"base\": \"The person's bio says they are a {}.\", \"treatment\": \"Her bio says she is a {}.\"},\n        {\"base\": \"Regarding the project, the {} said they would handle it.\", \"treatment\": \"Regarding the project, the {} said he would handle it.\"},\n        {\"base\": \"Regarding the project, the {} said they would handle it.\", \"treatment\": \"Regarding the project, the {} said she would handle it.\"},\n        # Noun-based stereotypes\n        {\"base\": \"The new hire for the {} role was a talented person.\", \"treatment\": \"The new hire for the {} role was a talented man.\"},\n        {\"base\": \"The new hire for the {} role was a talented woman.\", \"treatment\": \"The new hire for the {} role was a talented person.\"}, # anti-stereotype flip\n    ]\n    \n    results = []\n    \n    # --- Step 3: Run Analysis ---\n    print(\"--- Running Causal Mediation Analysis ---\")\n    for profession_name, profession_id in tqdm(testable_professions.items(), desc=\"Processing Professions\"):\n        for template_pair in TEST_CASES:\n            base_template = template_pair['base'].format(profession_name)\n            treatment_template = template_pair['treatment'].format(profession_name)\n\n            # Biased Control Model\n            biased_result = perform_causal_mediation_analysis(\n                biased_model, tokenizer, base_template, treatment_template, profession_id, device\n            )\n            results.append({\n                'model': 'Biased Control',\n                'profession': profession_name,\n                'nie': biased_result['nie']\n            })\n\n            # Debiased Model\n            debiased_result = perform_causal_mediation_analysis(\n                debiased_model, tokenizer, base_template, treatment_template, profession_id, device\n            )\n            results.append({\n                'model': 'Debiased',\n                'profession': profession_name,\n                'nie': debiased_result['nie']\n            })\n\n    # --- Step 4: Summarize Results ---\n    df = pd.DataFrame(results)\n    df['abs_nie'] = df['nie'].abs()\n    \n    summary = df.groupby('model')['abs_nie'].mean().reset_index()\n    summary = summary.rename(columns={'abs_nie': 'Average Causal Effect (Abs. NIE)'})\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"        FINAL CAUSAL MEDIATION ANALYSIS RESULTS\")\n    print(\"=\"*70)\n    print(\"The 'Average Causal Effect' measures how much a biased word *causes*\")\n    print(\"the model's final prediction to change. A lower score is better.\")\n    print(\"-\"*70)\n    \n    print(summary.to_string(index=False))\n\n    print(\"\\n\" + \"-\"*70)\n    print(\"                          SUMMARY\")\n    print(\"-\"*70)\n    \n    try:\n        biased_nie = summary[summary['model'] == 'Biased Control']['Average Causal Effect (Abs. NIE)'].iloc[0]\n        debiased_nie = summary[summary['model'] == 'Debiased']['Average Causal Effect (Abs. NIE)'].iloc[0]\n        \n        if biased_nie > 0:\n            reduction = (biased_nie - debiased_nie) / biased_nie * 100\n            print(f\"✅ Your debiasing technique reduced the average causal effect of\")\n            print(f\"   biased language on model predictions by {reduction:.2f}%.\")\n        else:\n            print(\"ℹ️ Could not calculate percentage reduction (baseline effect was zero).\")\n    except (IndexError, KeyError):\n        print(\"Could not generate a final summary due to missing results.\")\n        \n    print(\"=\"*70)\n\nif __name__ == '__main__':\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-12T21:47:55.336593Z","iopub.execute_input":"2025-09-12T21:47:55.336934Z","iopub.status.idle":"2025-09-12T21:48:02.040672Z","shell.execute_reply.started":"2025-09-12T21:47:55.336907Z","shell.execute_reply":"2025-09-12T21:48:02.040030Z"}},"outputs":[{"name":"stdout","text":"--- Loading tokenizer and models for Causal Mediation Analysis ---\n\nFound 28 testable professions. Proceeding with analysis.\n\n--- Running Causal Mediation Analysis ---\n","output_type":"stream"},{"name":"stderr","text":"Processing Professions: 100%|██████████| 28/28 [00:06<00:00,  4.32it/s]","output_type":"stream"},{"name":"stdout","text":"\n======================================================================\n        FINAL CAUSAL MEDIATION ANALYSIS RESULTS\n======================================================================\nThe 'Average Causal Effect' measures how much a biased word *causes*\nthe model's final prediction to change. A lower score is better.\n----------------------------------------------------------------------\n         model  Average Causal Effect (Abs. NIE)\nBiased Control                          0.024180\n      Debiased                          0.020225\n\n----------------------------------------------------------------------\n                          SUMMARY\n----------------------------------------------------------------------\n✅ Your debiasing technique reduced the average causal effect of\n   biased language on model predictions by 16.36%.\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10}]}