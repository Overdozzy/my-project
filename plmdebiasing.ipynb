{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-12T20:51:55.519729Z",
     "iopub.status.busy": "2025-09-12T20:51:55.519177Z",
     "iopub.status.idle": "2025-09-12T20:51:57.363043Z",
     "shell.execute_reply": "2025-09-12T20:51:57.362383Z",
     "shell.execute_reply.started": "2025-09-12T20:51:55.519709Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T20:51:57.364587Z",
     "iopub.status.busy": "2025-09-12T20:51:57.364194Z",
     "iopub.status.idle": "2025-09-12T20:53:09.377837Z",
     "shell.execute_reply": "2025-09-12T20:53:09.376881Z",
     "shell.execute_reply.started": "2025-09-12T20:51:57.364559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 1: Creating the Counterfactual Debiasing Dataset ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e19d52dc02e4e2589e5243fe810f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48205a6373a5469c9a7f32457a7bc34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ac041a9a86420f8da83103a5cda452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8a19736ba14a3d967085a1a7ba0f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer 'bert-base-uncased' loaded.\n",
      "Loading Bias in Bios dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a690cca7872a4a39b99cd679973e23eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13c494f511244b9bbd96ee212c39af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-0ab65b32c47407(â€¦):   0%|          | 0.00/64.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60984d411c37416f96283578e1f84dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001-5598c840ce8de1e(â€¦):   0%|          | 0.00/24.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352655422f7f4810ae9f3c31725c41d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/dev-00000-of-00001-e6551072fff26949(â€¦):   0%|          | 0.00/9.95M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5a6ed1e6c04ded8bf68f2c2c7643bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/257478 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c873fcf9cebe48d398be3d848b217e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/99069 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac844362b2c4485bd1839dd3dab94fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/39642 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd8f7a755f546afa751e18f0d79717d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2621b3d105745589c91e235c9d687e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset contains 9503 bios with gender words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CDA Pairs:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4018/9503 [00:16<00:20, 274.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (593 > 512). Running this sequence through the model will result in indexing errors\n",
      "Creating CDA Pairs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9503/9503 [00:35<00:00, 266.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… CDA dataset created with 19006 examples. Saved to cda_debiasing_dataset_BIAS_IN_BIOS.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 1: Data Preparation\n",
    "\n",
    "This script loads the 'Bias in Bios' dataset, filters it for biographies\n",
    "containing gendered words, and then creates a counterfactual version for\n",
    "each one by swapping the gendered terms (e.g., he -> she).\n",
    "\n",
    "This resulting dataset is the \"study material\" used to debias the model.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# This import is not strictly needed for the code but can be helpful for understanding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- NLTK Setup (run once) ---\n",
    "# FIX: Use LookupError instead of the deprecated nltk.downloader.DownloadError\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "try:\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('wordnet')\n",
    "try:\n",
    "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
    "except LookupError:\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def create_counterfactual(tokens, tokenizer_for_decoding, gender_swap_map, lemmatizer):\n",
    "    \"\"\"Swaps gendered tokens in a token list to create a counterfactual sentence.\"\"\"\n",
    "    new_tokens = tokens[:]\n",
    "    swapped = False\n",
    "    for i in range(len(new_tokens)):\n",
    "        token = new_tokens[i]\n",
    "        # Clean token for lookup (e.g., '##ing' -> 'ing')\n",
    "        clean_token = token.replace(\"##\", \"\")\n",
    "        if clean_token.isalpha():\n",
    "            lemma = lemmatizer.lemmatize(clean_token, get_wordnet_pos(clean_token))\n",
    "            if lemma in gender_swap_map:\n",
    "                swap_word = gender_swap_map[lemma]\n",
    "                # Preserve capitalization\n",
    "                if token[0].isupper():\n",
    "                    swap_word = swap_word.capitalize()\n",
    "                new_tokens[i] = swap_word\n",
    "                swapped = True\n",
    "    if not swapped:\n",
    "        return None\n",
    "    return tokenizer_for_decoding.convert_tokens_to_string(new_tokens)\n",
    "\n",
    "# --- Main Script ---\n",
    "def main():\n",
    "    print(\"--- STEP 1: Creating the Counterfactual Debiasing Dataset ---\")\n",
    "\n",
    "    # Gender lexicon for swapping\n",
    "    GENDER_PAIRS = {\n",
    "        \"he\": \"she\", \"him\": \"her\", \"his\": \"her\", \"himself\": \"herself\",\n",
    "        \"man\": \"woman\", \"boy\": \"girl\", \"male\": \"female\", \"father\": \"mother\",\n",
    "        \"son\": \"daughter\", \"brother\": \"sister\", \"husband\": \"wife\",\n",
    "        \"uncle\": \"aunt\", \"mr\": \"mrs\", \"sir\": \"madam\", \"king\": \"queen\", \"prince\": \"princess\"\n",
    "    }\n",
    "    full_gender_swap_map = GENDER_PAIRS.copy()\n",
    "    full_gender_swap_map.update({v: k for k, v in GENDER_PAIRS.items()})\n",
    "    GENDER_LEMMAS_SET = set(full_gender_swap_map.keys())\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Setup model and tokenizer (needed for tokenizing/detokenizing)\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"Tokenizer '{model_name}' loaded.\")\n",
    "\n",
    "    # --- Load Bias in Bios dataset ---\n",
    "    print(\"Loading Bias in Bios dataset...\")\n",
    "    # Using a larger subset for more effective training\n",
    "    dataset = load_dataset(\"LabHC/bias_in_bios\", split=\"train[:15000]\")\n",
    "\n",
    "    # --- Filter for gendered bios ---\n",
    "    def prepare_examples(example):\n",
    "        text = str(example['hard_text']).lower()\n",
    "        words = nltk.word_tokenize(text)\n",
    "        lemmas = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "        example['contains_gender'] = any(lemma in GENDER_LEMMAS_SET for lemma in lemmas)\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(prepare_examples, num_proc=2)\n",
    "    filtered_dataset = dataset.filter(lambda example: example['contains_gender'])\n",
    "    print(f\"Filtered dataset contains {len(filtered_dataset)} bios with gender words.\")\n",
    "\n",
    "    # --- Create counterfactual dataset ---\n",
    "    cda_pairs = []\n",
    "    for example in tqdm(filtered_dataset, desc=\"Creating CDA Pairs\"):\n",
    "        text = str(example['hard_text'])\n",
    "        label = str(example['profession'])\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        counterfactual_sentence = create_counterfactual(tokens, tokenizer, full_gender_swap_map, lemmatizer)\n",
    "\n",
    "        # Add original sentence\n",
    "        cda_pairs.append({'text': text, 'label': label})\n",
    "\n",
    "        # Add counterfactual if one was successfully created\n",
    "        if counterfactual_sentence:\n",
    "            cda_pairs.append({'text': counterfactual_sentence, 'label': label})\n",
    "\n",
    "    # --- Save CDA dataset to CSV ---\n",
    "    cda_df = pd.DataFrame(cda_pairs)\n",
    "    output_path = \"cda_debiasing_dataset_BIAS_IN_BIOS.csv\"\n",
    "    cda_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\nCDA dataset created with {len(cda_df)} examples. Saved to {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T20:53:09.379490Z",
     "iopub.status.busy": "2025-09-12T20:53:09.379184Z",
     "iopub.status.idle": "2025-09-12T21:12:12.974929Z",
     "shell.execute_reply": "2025-09-12T21:12:12.974279Z",
     "shell.execute_reply.started": "2025-09-12T20:53:09.379458Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-12 20:53:15.573414: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757710395.799950      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757710395.863420      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 2: Fine-Tuning the Model on the CDA Dataset ---\n",
      "Loading CDA debiasing dataset from 'cda_debiasing_dataset_BIAS_IN_BIOS.csv'...\n",
      "Found 28 unique profession labels.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0965ca67b3842b5ba564f696a1b423f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9827732efc1b497ca2a845481bbcdfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "015ad17a01eb41d684e2df98bb763e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2376' max='2376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2376/2376 18:29, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.557400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.182900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.850400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.747800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.626600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.533100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.494600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.441700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.386500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.358600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fine-tuning complete.\n",
      "\n",
      "Debiased model saved to './debiased_bert_model_bias_in_bios'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 2: Fine-Tuning (The Debiasing Process)\n",
    "\n",
    "This script loads the counterfactual dataset created in Step 1 and uses it\n",
    "to fine-tune the BERT model. The task is profession classification.\n",
    "\n",
    "By training on a dataset where gender is not predictive of the profession,\n",
    "the model learns to reduce its reliance on gendered words for this task,\n",
    "thus mitigating its bias.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset, Features, ClassLabel, Value\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "def main():\n",
    "    print(\"--- STEP 2: Fine-Tuning the Model on the CDA Dataset ---\")\n",
    "    \n",
    "    # --- SETUP ---\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # --- LOAD THE PREPARED CDA DATASET ---\n",
    "    cda_data_file = \"cda_debiasing_dataset_BIAS_IN_BIOS.csv\"\n",
    "    print(f\"Loading CDA debiasing dataset from '{cda_data_file}'...\")\n",
    "\n",
    "    df = pd.read_csv(cda_data_file)\n",
    "    df['label'] = df['label'].astype(int)\n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    num_labels = len(unique_labels)\n",
    "    print(f\"Found {num_labels} unique profession labels.\")\n",
    "\n",
    "    cleaned_file = \"cda_bias_in_bios_cleaned.csv\"\n",
    "    df.to_csv(cleaned_file, index=False)\n",
    "\n",
    "    # Define features for the HuggingFace dataset loader\n",
    "    cda_features = Features({\n",
    "        'text': Value('string'),\n",
    "        'label': ClassLabel(num_classes=num_labels, names=[str(x) for x in unique_labels])\n",
    "    })\n",
    "\n",
    "    cda_dataset = load_dataset('csv', data_files=cleaned_file, features=cda_features, split='train')\n",
    "\n",
    "    # --- TOKENIZE DATASET ---\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "    tokenized_dataset = cda_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # --- MODEL SETUP ---\n",
    "       # --- MODEL SETUP ---\n",
    "    # Define the official list of all possible professions to ensure consistency\n",
    "    all_profession_names = [\n",
    "        'accountant', 'architect', 'attorney', 'chiropractor', 'comedian', \n",
    "        'composer', 'dentist', 'dietitian', 'dj', 'filmmaker', \n",
    "        'interior_designer', 'journalist', 'model', 'nurse', 'painter', \n",
    "        'paralegal', 'pastor', 'personal_trainer', 'photographer', 'physician', \n",
    "        'poet', 'professor', 'psychologist', 'rapper', 'software_engineer', \n",
    "        'surgeon', 'teacher', 'yoga_instructor'\n",
    "    ]\n",
    "    \n",
    "    # Create the label mapping for the model's configuration\n",
    "    id2label = {i: name for i, name in enumerate(all_profession_names)}\n",
    "    label2id = {name: i for i, name in enumerate(all_profession_names)}\n",
    "    \n",
    "    # Load the model with the correct label mappings\n",
    "    model_to_finetune = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(all_profession_names),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    model_to_finetune.to(device)\n",
    "    # --- TRAINING ARGUMENTS ---\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_bias_in_bios\",\n",
    "        num_train_epochs=2, # Keep epochs low to avoid catastrophic forgetting\n",
    "        per_device_train_batch_size=8,\n",
    "        learning_rate=2e-5, # A smaller learning rate is crucial for fine-tuning\n",
    "        logging_dir='./logs_bias_in_bios',\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model_to_finetune,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "    )\n",
    "\n",
    "    print(\"\\nðŸš€ Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"âœ… Fine-tuning complete.\")\n",
    "\n",
    "    # --- SAVE THE DEBIASED MODEL ---\n",
    "    debiased_model_path = \"./debiased_bert_model_bias_in_bios\"\n",
    "    trainer.save_model(debiased_model_path)\n",
    "    tokenizer.save_pretrained(debiased_model_path)\n",
    "    print(f\"\\nDebiased model saved to '{debiased_model_path}'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T21:12:12.976376Z",
     "iopub.status.busy": "2025-09-12T21:12:12.976167Z",
     "iopub.status.idle": "2025-09-12T21:12:20.715818Z",
     "shell.execute_reply": "2025-09-12T21:12:20.715044Z",
     "shell.execute_reply.started": "2025-09-12T21:12:12.976360Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 3 (FINAL): Evaluating Bias in the BASELINE Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16fe0bbf8f449c59693a0bcf2865dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41940b1026d1403183aa9c159e6279a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "intrasentence/validation-00000-of-00001.(â€¦):   0%|          | 0.00/599k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c424c0d2a2499391516005a6b9219c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2106 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Baseline Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2106/2106 [00:03<00:00, 570.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final baseline evaluation complete. Results saved to baseline_bias_results_FINAL.csv\n",
      "\n",
      "============================================================\n",
      "           FINAL BASELINE MODEL BIAS SCORES\n",
      "============================================================\n",
      "Metric 1: Avg. Absolute Probability Difference: 0.0522\n",
      "Metric 2: Avg. Absolute Log-Odds Difference:   1.4432\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3 (REVISED): Classification-Based Bias Evaluation (Baseline Model)\n",
    "\n",
    "This script evaluates the BIASED CONTROL model using a classification-based probe.\n",
    "It measures how the model's probability for a profession changes when gendered\n",
    "language is introduced into a neutral template.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a standard set of templates for the evaluation\n",
    "TEMPLATES = [\n",
    "    # (neutral_base, treatment_male, treatment_female)\n",
    "    (\"The person's bio says they are a {}.\", \"His bio says he is a {}.\", \"Her bio says she is a {}.\"),\n",
    "    (\"Regarding the project, the {} said they would handle it.\", \"Regarding the project, the {} said he would handle it.\", \"Regarding the project, the {} said she would handle it.\"),\n",
    "    (\"The new hire for the {} role was a talented person.\", \"The new hire for the {} role was a talented man.\", \"The new hire for the {} role was a talented woman.\"),\n",
    "]\n",
    "\n",
    "def get_prob_of_label(model, tokenizer, text, label_id, device):\n",
    "    \"\"\"Calculates the softmax probability for a specific label for a given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "    return float(probs[label_id].item())\n",
    "\n",
    "def run_classification_evaluation(model_path, output_csv):\n",
    "    \"\"\"Loads a classifier and runs the bias evaluation, saving results to CSV.\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"--- Loading model for classification evaluation: {model_path} ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device).eval()\n",
    "\n",
    "    if not model.config.id2label:\n",
    "        print(f\" ERROR: Model at {model_path} is missing the id2label mapping!\")\n",
    "        return\n",
    "\n",
    "    # Align by label NAME to get the correct index (label_id) for the model\n",
    "    id2label = {int(k): v for k, v in model.config.id2label.items()}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "    profession_names = sorted(label2id.keys())\n",
    "\n",
    "    results = []\n",
    "    for name in tqdm(profession_names, desc=\"Evaluating professions\"):\n",
    "        label_id = label2id[name]\n",
    "        prob_deltas, log_odds_deltas = [], []\n",
    "        \n",
    "        for base_template, treat_m_template, treat_f_template in TEMPLATES:\n",
    "            base_text = base_template.format(name)\n",
    "            p_base = get_prob_of_label(model, tokenizer, base_text, label_id, device)\n",
    "            \n",
    "            # Evaluate for both male and female treatments\n",
    "            for treat_template in (treat_m_template, treat_f_template):\n",
    "                treat_text = treat_template.format(name)\n",
    "                p_treat = get_prob_of_label(model, tokenizer, treat_text, label_id, device)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                prob_deltas.append(p_treat - p_base)\n",
    "                log_odds_deltas.append(math.log(p_treat + 1e-12) - math.log(p_base + 1e-12))\n",
    "\n",
    "        results.append({\n",
    "            'profession': name,\n",
    "            'avg_prob_delta': float(np.mean(prob_deltas)),\n",
    "            'avg_log_odds_delta': float(np.mean(log_odds_deltas))\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"âœ… Saved classification-based bias results to {output_csv}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_classification_evaluation(\n",
    "        model_path='./biased_control_bert_model',\n",
    "        output_csv='baseline_bias_results_cls.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T21:12:20.718611Z",
     "iopub.status.busy": "2025-09-12T21:12:20.718197Z",
     "iopub.status.idle": "2025-09-12T21:12:25.839416Z",
     "shell.execute_reply": "2025-09-12T21:12:25.838646Z",
     "shell.execute_reply.started": "2025-09-12T21:12:20.718593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./debiased_bert_model_bias_in_bios and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 4 (FINAL): Evaluating Bias in the DEBIASED Model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Debiased Model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2106/2106 [00:03<00:00, 578.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final debiased evaluation complete. Results saved to debiased_bias_results_FINAL.csv\n",
      "\n",
      "============================================================\n",
      "           FINAL DEBIASED MODEL BIAS SCORES\n",
      "============================================================\n",
      "Metric 1: Avg. Absolute Probability Difference: 0.0000\n",
      "Metric 2: Avg. Absolute Log-Odds Difference:   0.7099\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 4 (FINAL, CLASSIFICATION-BASED):\n",
    "Evaluate bias in the DEBIASED classifier using the same templates and metrics\n",
    "as the baseline in Step 3.\n",
    "\n",
    "Outputs: debiased_bias_results_cls.csv with per-profession average deltas.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "TEMPLATES = [\n",
    "    (\"The person's bio says they are a {}.\", \"His bio says he is a {}.\", \"Her bio says she is a {}.\"),\n",
    "    (\"Regarding the project, the {} said they would handle it.\",\n",
    "     \"Regarding the project, the {} said he would handle it.\",\n",
    "     \"Regarding the project, the {} said she would handle it.\"),\n",
    "    (\"The new hire for the {} role was a talented person.\",\n",
    "     \"The new hire for the {} role was a talented man.\",\n",
    "     \"The new hire for the {} role was a talented woman.\"),\n",
    "]\n",
    "\n",
    "def prob_of_label(model, tokenizer, text, label_id, device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1).squeeze()\n",
    "    return float(probs[label_id].item())\n",
    "\n",
    "def run_cls_eval(model_path, out_csv):\n",
    "    print(\"--- STEP 4 (FINAL, CLS): Evaluating Bias in the DEBIASED Classifier ---\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path).to(device).eval()\n",
    "\n",
    "    id2label = {int(k): v for k, v in model.config.id2label.items()}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "    profession_names = sorted(label2id.keys())\n",
    "\n",
    "    rows = []\n",
    "    for name in profession_names:\n",
    "        label_id = label2id[name]\n",
    "        deltas = []\n",
    "        logodds = []\n",
    "        for base, treat_m, treat_f in TEMPLATES:\n",
    "            base_text = base.format(name)\n",
    "            for treat in (treat_m, treat_f):\n",
    "                treat_text = treat.format(name)\n",
    "                p_base = prob_of_label(model, tokenizer, base_text, label_id, device)\n",
    "                p_treat = prob_of_label(model, tokenizer, treat_text, label_id, device)\n",
    "                delta = p_treat - p_base\n",
    "                deltas.append(delta)\n",
    "                logodds.append(math.log(p_treat + 1e-12) - math.log(p_base + 1e-12))\n",
    "\n",
    "        rows.append({\n",
    "            'profession': name,\n",
    "            'avg_prob_delta': float(np.mean(deltas)),\n",
    "            'avg_log_odds_delta': float(np.mean(logodds))\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"\\nSaved classification-based debiased bias results to {out_csv}\")\n",
    "\n",
    "    # Summary\n",
    "    avg_abs_prob = df['avg_prob_delta'].abs().mean()\n",
    "    avg_abs_log = df['avg_log_odds_delta'].abs().mean()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"      FINAL DEBIASED CLASSIFIER BIAS SCORES\\n\")\n",
    "    print(f\"Metric 1: Avg. Absolute Probability Delta: {avg_abs_prob:.4f}\")\n",
    "    print(f\"Metric 2: Avg. Absolute Log-Odds Delta:   {avg_abs_log:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_cls_eval('./debiased_bert_model_bias_in_bios', 'debiased_bias_results_cls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T21:12:25.840804Z",
     "iopub.status.busy": "2025-09-12T21:12:25.840584Z",
     "iopub.status.idle": "2025-09-12T21:12:25.874734Z",
     "shell.execute_reply": "2025-09-12T21:12:25.874189Z",
     "shell.execute_reply.started": "2025-09-12T21:12:25.840787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STEP 5 (FINAL): Performing Statistical Significance Tests ---\n",
      "Found 56 paired examples to compare.\n",
      "\n",
      "======================================================================\n",
      "        FINAL STATISTICAL SIGNIFICANCE TEST RESULTS\n",
      "======================================================================\n",
      "\n",
      "--- Metric 1: Probability Difference ---\n",
      "Baseline Avg. Absolute Score: 0.0522\n",
      "Debiased Avg. Absolute Score: 0.0000\n",
      "T-statistic: 3.2324, P-value: 0.002076\n",
      "âœ… Result is STATISTICALLY SIGNIFICANT.\n",
      "\n",
      "--- Metric 2: Log-Odds Difference ---\n",
      "Baseline Avg. Absolute Score: 1.4432\n",
      "Debiased Avg. Absolute Score: 0.7099\n",
      "T-statistic: 3.5250, P-value: 0.0008616\n",
      "âœ… Result is STATISTICALLY SIGNIFICANT.\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 5 (FINAL, CLASSIFICATION-BASED):\n",
    "Statistical significance testing for BOTH bias metrics on classifier outputs.\n",
    "\n",
    "This script loads the results from the baseline (Step 3) and debiased (Step 4)\n",
    "classification-based evaluations and performs a paired t-test on both the\n",
    "Probability Delta and the Log-Odds Delta metrics (absolute values).\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def main():\n",
    "    print(\"--- STEP 5 (FINAL, CLS): Performing Statistical Significance Tests ---\")\n",
    "\n",
    "    baseline_results_file = \"baseline_bias_results_cls.csv\"\n",
    "    debiased_results_file = \"debiased_bias_results_cls.csv\"\n",
    "\n",
    "    try:\n",
    "        df_baseline = pd.read_csv(baseline_results_file)\n",
    "        df_debiased = pd.read_csv(debiased_results_file)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" ERROR: Could not find a results file: {e.filename}\")\n",
    "        return\n",
    "\n",
    "    # Merge by profession\n",
    "    comparison_df = pd.merge(\n",
    "        df_baseline, df_debiased,\n",
    "        on=['profession'],\n",
    "        suffixes=('_baseline', '_debiased')\n",
    "    )\n",
    "    print(f\"Found {len(comparison_df)} professions to compare.\")\n",
    "\n",
    "    # --- Test 1: Probability Delta Metric ---\n",
    "    baseline_prob_scores = comparison_df['avg_prob_delta_baseline'].abs()\n",
    "    debiased_prob_scores = comparison_df['avg_prob_delta_debiased'].abs()\n",
    "    t_stat_prob, p_val_prob = stats.ttest_rel(baseline_prob_scores, debiased_prob_scores)\n",
    "\n",
    "    # --- Test 2: Log-Odds Delta Metric ---\n",
    "    baseline_log_scores = comparison_df['avg_log_odds_delta_baseline'].abs()\n",
    "    debiased_log_scores = comparison_df['avg_log_odds_delta_debiased'].abs()\n",
    "    t_stat_log, p_val_log = stats.ttest_rel(baseline_log_scores, debiased_log_scores)\n",
    "\n",
    "    # --- Print Results ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"        FINAL STATISTICAL SIGNIFICANCE TEST RESULTS (CLS)\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"\\n--- Metric 1: Probability Delta ---\")\n",
    "    print(f\"Baseline Avg. Absolute Score: {baseline_prob_scores.mean():.4f}\")\n",
    "    print(f\"Debiased Avg. Absolute Score: {debiased_prob_scores.mean():.4f}\")\n",
    "    print(f\"T-statistic: {t_stat_prob:.4f}, P-value: {p_val_prob:.4g}\")\n",
    "    if p_val_prob < 0.05:\n",
    "        print(\" Result is STATISTICALLY SIGNIFICANT.\")\n",
    "    else:\n",
    "        print(\"  Result is NOT statistically significant.\")\n",
    "    \n",
    "    print(\"\\n--- Metric 2: Log-Odds Delta ---\")\n",
    "    print(f\"Baseline Avg. Absolute Score: {baseline_log_scores.mean():.4f}\")\n",
    "    print(f\"Debiased Avg. Absolute Score: {debiased_log_scores.mean():.4f}\")\n",
    "    print(f\"T-statistic: {t_stat_log:.4f}, P-value: {p_val_log:.4g}\")\n",
    "    if p_val_log < 0.05:\n",
    "        print(\" Result is STATISTICALLY SIGNIFICANT.\")\n",
    "    else:\n",
    "        print(\" Result is NOT statistically significant.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T21:12:25.875595Z",
     "iopub.status.busy": "2025-09-12T21:12:25.875421Z",
     "iopub.status.idle": "2025-09-12T21:47:11.623146Z",
     "shell.execute_reply": "2025-09-12T21:47:11.622228Z",
     "shell.execute_reply.started": "2025-09-12T21:12:25.875581Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found counterfactual dataset with 19006 examples.\n",
      "The Biased Control model will be trained on this many examples.\n",
      "\n",
      "Loading the first 19006 examples from the original 'Bias in Bios' dataset...\n",
      "Using a hard-coded list of professions to ensure robustness...\n",
      "Found 28 unique professions in this subset of 19006 examples.\n",
      "Model will be configured for all 28 possible professions.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f18141b027431ebfecf84f25dcc7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19006 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting fine-tuning on SIZE-MATCHED BIASED dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2376' max='2376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2376/2376 34:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.067500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.941000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.773100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.732200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.748000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.471200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.361600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.399700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Fine-tuning of biased control model complete.\n",
      "Size-matched biased control model saved to ./biased_control_bert_model\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 6 (REVISED): Train a Size-Matched Biased Control Model\n",
    "\n",
    "This script implements the crucial step of training the \"Biased Control\" model\n",
    "on a dataset that is perfectly size-matched to the counterfactual dataset.\n",
    "\n",
    "This ensures a true \"apples-to-apples\" comparison in the final evaluation,\n",
    "isolating the debiasing technique as the only variable.\n",
    "\n",
    "THE PROCESS:\n",
    "1.  It first loads your counterfactual dataset (`cda_debiasing_dataset...csv`)\n",
    "    to determine its exact size (N).\n",
    "2.  It then loads the original, biased `Bias in Bios` dataset, but takes only\n",
    "    the first N examples.\n",
    "3.  It fine-tunes a fresh BERT model on this size-matched, biased dataset.\n",
    "4.  The resulting model is the perfect control for the final evaluations.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset, Features, ClassLabel, Value, Dataset, load_dataset_builder\n",
    "import os\n",
    "\n",
    "def main():\n",
    "    # --- 1. Determine the size of the counterfactual dataset ---\n",
    "    CDA_DATA_FILE = \"cda_debiasing_dataset_BIAS_IN_BIOS.csv\"\n",
    "    if not os.path.exists(CDA_DATA_FILE):\n",
    "        print(f\" ERROR: Counterfactual dataset not found at '{CDA_DATA_FILE}'.\")\n",
    "        print(\"Please run script '1_create_cda_dataset.py' first.\")\n",
    "        return\n",
    "        \n",
    "    cda_df = pd.read_csv(CDA_DATA_FILE)\n",
    "    target_dataset_size = len(cda_df)\n",
    "    print(f\"Found counterfactual dataset with {target_dataset_size} examples.\")\n",
    "    print(\"The Biased Control model will be trained on this many examples.\")\n",
    "\n",
    "    # --- 2. Setup Tokenizer ---\n",
    "    # We can use a fresh tokenizer or one from the debiased model path.\n",
    "    # Using a fresh one is cleanest for this control model.\n",
    "    MODEL_NAME = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # --- 3. Load and slice the original Bias in Bios dataset ---\n",
    "    print(f\"\\nLoading the first {target_dataset_size} examples from the original 'Bias in Bios' dataset...\")\n",
    "    original_dataset = load_dataset(\"LabHC/bias_in_bios\", split=f\"train[:{target_dataset_size}]\")\n",
    "\n",
    "    df = original_dataset.to_pandas()\n",
    "    df['text'] = df['hard_text'].astype(str)\n",
    "    df['label'] = df['profession'].astype(int)\n",
    "    \n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    num_labels = len(unique_labels)\n",
    "    \n",
    "    # Get the official label names for the full dataset to create a consistent config\n",
    "    # The datasets library is having trouble inferring the ClassLabel type.\n",
    "    # To fix this, we will hard-code the known list of 28 professions.\n",
    "    print(\"Using a hard-coded list of professions to ensure robustness...\")\n",
    "    all_profession_names = [\n",
    "        'accountant', 'architect', 'attorney', 'chiropractor', 'comedian', \n",
    "        'composer', 'dentist', 'dietitian', 'dj', 'filmmaker', \n",
    "        'interior_designer', 'journalist', 'model', 'nurse', 'painter', \n",
    "        'paralegal', 'pastor', 'personal_trainer', 'photographer', 'physician', \n",
    "        'poet', 'professor', 'psychologist', 'rapper', 'software_engineer', \n",
    "        'surgeon', 'teacher', 'yoga_instructor'\n",
    "    ]\n",
    "    \n",
    "    # The model needs to know the full set of possible labels, even if not all are in this subset\n",
    "    full_num_labels = len(all_profession_names)\n",
    "    \n",
    "    # Create the label mapping for the model's configuration\n",
    "    id2label = {i: name for i, name in enumerate(all_profession_names)}\n",
    "    label2id = {name: i for i, name in enumerate(all_profession_names)}\n",
    "    \n",
    "    print(f\"Found {num_labels} unique professions in this subset of {target_dataset_size} examples.\")\n",
    "    print(f\"Model will be configured for all {full_num_labels} possible professions.\")\n",
    "\n",
    "    # --- 4. Prepare dataset for Hugging Face Trainer ---\n",
    "    features = Features({\n",
    "        'text': Value('string'),\n",
    "        'label': ClassLabel(num_classes=full_num_labels, names=all_profession_names)\n",
    "    })\n",
    "    \n",
    "    prepared_df = df[['text', 'label']]\n",
    "    biased_hf_dataset = Dataset.from_pandas(prepared_df, features=features)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    tokenized_dataset = biased_hf_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    # --- 5. Train the Biased Control Model ---\n",
    "    model_to_finetune = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=full_num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    BIASED_MODEL_PATH = \"./biased_control_bert_model\"\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_biased_control_sized\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=8,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    trainer = Trainer(model=model_to_finetune, args=training_args, train_dataset=tokenized_dataset)\n",
    "\n",
    "    print(\"\\n Starting fine-tuning on SIZE-MATCHED BIASED dataset...\")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning of biased control model complete.\")\n",
    "    \n",
    "    trainer.save_model(BIASED_MODEL_PATH)\n",
    "    tokenizer.save_pretrained(BIASED_MODEL_PATH)\n",
    "    print(f\"Size-matched biased control model saved to {BIASED_MODEL_PATH}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-12T21:47:55.336934Z",
     "iopub.status.busy": "2025-09-12T21:47:55.336593Z",
     "iopub.status.idle": "2025-09-12T21:48:02.040672Z",
     "shell.execute_reply": "2025-09-12T21:48:02.040030Z",
     "shell.execute_reply.started": "2025-09-12T21:47:55.336907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading tokenizer and models for Causal Mediation Analysis ---\n",
      "\n",
      "Found 28 testable professions. Proceeding with analysis.\n",
      "\n",
      "--- Running Causal Mediation Analysis ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Professions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:06<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "        FINAL CAUSAL MEDIATION ANALYSIS RESULTS\n",
      "======================================================================\n",
      "The 'Average Causal Effect' measures how much a biased word *causes*\n",
      "the model's final prediction to change. A lower score is better.\n",
      "----------------------------------------------------------------------\n",
      "         model  Average Causal Effect (Abs. NIE)\n",
      "Biased Control                          0.024180\n",
      "      Debiased                          0.020225\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "                          SUMMARY\n",
      "----------------------------------------------------------------------\n",
      "âœ… Your debiasing technique reduced the average causal effect of\n",
      "   biased language on model predictions by 16.36%.\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 7 (FINAL DISSERTATION ANALYSIS): Causal Mediation Analysis\n",
    "\n",
    "This script estimates the *causal effect* of gendered language on model predictions.\n",
    "\n",
    "It goes beyond correlation to measure how much biased words (e.g., \"his\", \"her\",\n",
    "\"man\", \"woman\") *cause* the model to change its output probability.\n",
    "\n",
    "THE EXPERIMENT:\n",
    "1.  Define pairs of templates: a neutral \"base\" sentence and a gendered \"treatment\".\n",
    "    Example:\n",
    "      Base: \"The person's bio says they are a doctor.\"\n",
    "      Treatment: \"His bio says he is a doctor.\"\n",
    "2.  For each profession label:\n",
    "      a) Total Effect = model probability with biased sentence.\n",
    "      b) Direct Effect = model probability with neutral sentence.\n",
    "      c) Natural Indirect Effect (NIE) = difference (a - b).\n",
    "         â†’ This captures the *causal impact* of the biased word.\n",
    "3.  Compare the average NIE for:\n",
    "      - Biased Control Model (trained on original data).\n",
    "      - Debiased Model (trained on counterfactual data).\n",
    "\"\"\"\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CORE CAUSAL MEDIATION ANALYSIS FUNCTION\n",
    "# ==============================================================================\n",
    "def perform_causal_mediation_analysis(model, tokenizer, base_template, treatment_template, profession_label_id, device):\n",
    "    \"\"\"\n",
    "    Performs a single Causal Mediation Analysis test for one profession.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # --- a) Total Effect: probability under biased treatment sentence ---\n",
    "        treatment_inputs = tokenizer(treatment_template, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "        treatment_outputs = model(**treatment_inputs)\n",
    "        treatment_probs = torch.softmax(treatment_outputs.logits, dim=-1).squeeze()\n",
    "        total_effect_prob = treatment_probs[profession_label_id].item()\n",
    "\n",
    "        # --- b) Direct Effect: probability under neutral base sentence ---\n",
    "        base_inputs = tokenizer(base_template, return_tensors=\"pt\", truncation=True, max_length=128).to(device)\n",
    "        base_outputs = model(**base_inputs)\n",
    "        base_probs = torch.softmax(base_outputs.logits, dim=-1).squeeze()\n",
    "        direct_effect_prob = base_probs[profession_label_id].item()\n",
    "\n",
    "        # --- c) Natural Indirect Effect (NIE) ---\n",
    "        nie = total_effect_prob - direct_effect_prob\n",
    "\n",
    "    return {\n",
    "        'total_effect_prob': total_effect_prob,\n",
    "        'direct_effect_prob': direct_effect_prob,\n",
    "        'nie': nie\n",
    "    }\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. MAIN SCRIPT EXECUTION\n",
    "# ==============================================================================\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # --- Paths to saved fine-tuned models ---\n",
    "    BIASED_CONTROL_MODEL_PATH = \"./biased_control_bert_model\"\n",
    "    DEBIASED_MODEL_PATH = \"./debiased_bert_model_bias_in_bios\"\n",
    "    \n",
    "    if not os.path.exists(BIASED_CONTROL_MODEL_PATH) or not os.path.exists(DEBIASED_MODEL_PATH):\n",
    "        print(\" ERROR: One or both of the required models were not found.\")\n",
    "        print(f\"Please ensure '{BIASED_CONTROL_MODEL_PATH}' and '{DEBIASED_MODEL_PATH}' exist.\")\n",
    "        return\n",
    "\n",
    "    print(\"--- Loading tokenizer and models for Causal Mediation Analysis ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(DEBIASED_MODEL_PATH)\n",
    "    biased_model = AutoModelForSequenceClassification.from_pretrained(BIASED_CONTROL_MODEL_PATH)\n",
    "    debiased_model = AutoModelForSequenceClassification.from_pretrained(DEBIASED_MODEL_PATH)\n",
    "\n",
    "    # --- Step 1: Extract profession mapping directly from model config ---\n",
    "    # The config's id2label map is the most reliable source of truth.\n",
    "    # e.g., config.id2label = { 0: \"nurse\", 1: \"engineer\", ... }\n",
    "    model_id2label = {int(k): v for k, v in debiased_model.config.id2label.items()}\n",
    "    testable_professions = {name: idx for idx, name in model_id2label.items()}\n",
    "    \n",
    "    if not testable_professions:\n",
    "        print(\"\\n ERROR: No profession labels found in the model's config.json.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nFound {len(testable_professions)} testable professions. Proceeding with analysis.\\n\")\n",
    "\n",
    "    # --- Step 2: Define Template Pairs ---\n",
    "    TEST_CASES = [\n",
    "        # Pronoun-based stereotypes\n",
    "        {\"base\": \"The person's bio says they are a {}.\", \"treatment\": \"His bio says he is a {}.\"},\n",
    "        {\"base\": \"The person's bio says they are a {}.\", \"treatment\": \"Her bio says she is a {}.\"},\n",
    "        {\"base\": \"Regarding the project, the {} said they would handle it.\", \"treatment\": \"Regarding the project, the {} said he would handle it.\"},\n",
    "        {\"base\": \"Regarding the project, the {} said they would handle it.\", \"treatment\": \"Regarding the project, the {} said she would handle it.\"},\n",
    "        # Noun-based stereotypes\n",
    "        {\"base\": \"The new hire for the {} role was a talented person.\", \"treatment\": \"The new hire for the {} role was a talented man.\"},\n",
    "        {\"base\": \"The new hire for the {} role was a talented woman.\", \"treatment\": \"The new hire for the {} role was a talented person.\"}, # anti-stereotype flip\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # --- Step 3: Run Analysis ---\n",
    "    print(\"--- Running Causal Mediation Analysis ---\")\n",
    "    for profession_name, profession_id in tqdm(testable_professions.items(), desc=\"Processing Professions\"):\n",
    "        for template_pair in TEST_CASES:\n",
    "            base_template = template_pair['base'].format(profession_name)\n",
    "            treatment_template = template_pair['treatment'].format(profession_name)\n",
    "\n",
    "            # Biased Control Model\n",
    "            biased_result = perform_causal_mediation_analysis(\n",
    "                biased_model, tokenizer, base_template, treatment_template, profession_id, device\n",
    "            )\n",
    "            results.append({\n",
    "                'model': 'Biased Control',\n",
    "                'profession': profession_name,\n",
    "                'nie': biased_result['nie']\n",
    "            })\n",
    "\n",
    "            # Debiased Model\n",
    "            debiased_result = perform_causal_mediation_analysis(\n",
    "                debiased_model, tokenizer, base_template, treatment_template, profession_id, device\n",
    "            )\n",
    "            results.append({\n",
    "                'model': 'Debiased',\n",
    "                'profession': profession_name,\n",
    "                'nie': debiased_result['nie']\n",
    "            })\n",
    "\n",
    "    # --- Step 4: Summarize Results ---\n",
    "    df = pd.DataFrame(results)\n",
    "    df['abs_nie'] = df['nie'].abs()\n",
    "    \n",
    "    summary = df.groupby('model')['abs_nie'].mean().reset_index()\n",
    "    summary = summary.rename(columns={'abs_nie': 'Average Causal Effect (Abs. NIE)'})\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"        FINAL CAUSAL MEDIATION ANALYSIS RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"The 'Average Causal Effect' measures how much a biased word *causes*\")\n",
    "    print(\"the model's final prediction to change. A lower score is better.\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"                          SUMMARY\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    try:\n",
    "        biased_nie = summary[summary['model'] == 'Biased Control']['Average Causal Effect (Abs. NIE)'].iloc[0]\n",
    "        debiased_nie = summary[summary['model'] == 'Debiased']['Average Causal Effect (Abs. NIE)'].iloc[0]\n",
    "        \n",
    "        if biased_nie > 0:\n",
    "            reduction = (biased_nie - debiased_nie) / biased_nie * 100\n",
    "            print(f\" Your debiasing technique reduced the average causal effect of\")\n",
    "            print(f\"   biased language on model predictions by {reduction:.2f}%.\")\n",
    "        else:\n",
    "            print(\"Could not calculate percentage reduction (baseline effect was zero).\")\n",
    "    except (IndexError, KeyError):\n",
    "        print(\"Could not generate a final summary due to missing results.\")\n",
    "        \n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
