"""
This script fine-tunes a pre-trained BERT model on the counterfactual dataset
generated by the previous step. The objective is to train a profession
classification model that exhibits reduced gender bias. The final fine-tuned
model is saved to a local directory for subsequent evaluation.
"""
import pandas as pd
import torch
from datasets import load_dataset, Features, ClassLabel, Value, Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer
)
import os

def main():
    # --- 1. Configuration ---
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    MODEL_NAME = "bert-base-uncased"
    CDA_DATA_FILE = "cda_debiasing_dataset_BIAS_IN_BIOS.csv"
    DEBIASED_MODEL_PATH = "./debiased_bert_model_bias_in_bios"

    if not os.path.exists(CDA_DATA_FILE):
        print(f"Error: Counterfactual dataset not found at '{CDA_DATA_FILE}'.")
        print("Please run '1_create_cda_dataset.py' first.")
        return
        
    print(f"Loading counterfactual dataset from '{CDA_DATA_FILE}'...")
    df = pd.read_csv(CDA_DATA_FILE)

    # --- 2. Dataset Preparation ---
    # Use the official list of 28 professions for label consistency.
    all_profession_names = [
        'accountant', 'architect', 'attorney', 'chiropractor', 'comedian', 
        'composer', 'dentist', 'dietitian', 'dj', 'filmmaker', 
        'interior_designer', 'journalist', 'lawyer', 'model', 'nurse', 
        'painter', 'paralegal', 'pastor', 'personal_trainer', 'photographer', 
        'physician', 'poet', 'professor', 'psychologist', 'rapper', 
        'software_engineer', 'surgeon', 'teacher'
    ]
    full_num_labels = len(all_profession_names)
    print(f"Configuring model for {full_num_labels} profession labels.")

    id2label = {i: name for i, name in enumerate(all_profession_names)}
    label2id = {name: i for i, name in enumerate(all_profession_names)}

    cda_features = Features({
        'text': Value('string'),
        'label': ClassLabel(num_classes=full_num_labels, names=all_profession_names)
    })
    
    df['label'] = df['label'].astype(int)
    hf_dataset = Dataset.from_pandas(df, features=cda_features)
    
    # --- 3. Tokenization ---
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    def tokenize_function(examples):
        return tokenizer(examples['text'], padding="max_length", truncation=True, max_length=128)

    print("Tokenizing dataset...")
    tokenized_dataset = hf_dataset.map(tokenize_function, batched=True)

    # --- 4. Model Training ---
    model_to_finetune = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=full_num_labels,
        id2label=id2label,
        label2id=label2id
    )
    model_to_finetune.to(device)

    training_args = TrainingArguments(
        output_dir="./results_debiased",
        num_train_epochs=2,
        per_device_train_batch_size=8,
        logging_dir='./logs_debiased',
        logging_steps=100,
        save_strategy="epoch",
        report_to="none"
    )

    trainer = Trainer(
        model=model_to_finetune,
        args=training_args,
        train_dataset=tokenized_dataset,
    )

    print("Starting model fine-tuning...")
    trainer.train()
    
    # --- 5. Save Model ---
    print("Fine-tuning complete.")
    trainer.save_model(DEBIASED_MODEL_PATH)
    tokenizer.save_pretrained(DEBIASED_MODEL_PATH)
    
    print(f"Debiased model saved to: {DEBIASED_MODEL_PATH}")

if __name__ == '__main__':
    main()

